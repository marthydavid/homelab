additionalPrometheusRulesMap: {}
alertmanager:
  ingress:
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
    enabled: true
    ingressClassName: nginx
    hosts:
      - alertmanager.bp-rke2.marthy.xyz
    tls:
      - secretName: alertmanager-tls-secret
        hosts:
          - alertmanager.bp-rke2.marthy.xyz
  alertmanagerSpec:
    image:
      repository: quay.io/prometheus/alertmanager
      sha: ''
      tag: v0.24.0
    replicas: 1
    resources:
      limits:
        cpu: 150m
        memory: 100Mi
      requests:
        cpu: 100m
        memory: 25Mi
    retention: 120h
    routePrefix: /
    secrets: []
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
    storage:
      volumeClaimTemplate:
        metadata:
          name: rancher-alertmanager
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 1Gi
    tolerations: []
    topologySpreadConstraints: []
    useExistingSecret: true
    volumeMounts: []
    volumes: []
    configSecret: alertmanager-rancher-monitoring-alertmanager

  annotations: {}
  apiVersion: v2
  config:
    global:
      resolve_timeout: 5m
    receivers:
      - name: 'null'
    route:
      group_by:
        - job
      group_interval: 5m
      group_wait: 30s
      receiver: 'null'
      repeat_interval: 12h
      routes:
        - match:
            alertname: Watchdog
          receiver: 'null'
    templates:
      - /etc/alertmanager/config/*.tmpl
  enabled: true
  templateFiles:
    rancher_defaults.tmpl: >-
      {{- define "slack.rancher.text" -}}

      {{ template "rancher.text_multiple" . }}

      {{- end -}}


      {{- define "rancher.text_multiple" -}}

      *[GROUP - Details]*

      One or more alarms in this group have triggered a notification.


      {{- if gt (len .GroupLabels.Values) 0 }}

      *Group Labels:*
        {{- range .GroupLabels.SortedPairs }}
        • *{{ .Name }}:* `{{ .Value }}`
        {{- end }}
      {{- end }}

      {{- if .ExternalURL }}

      *Link to AlertManager:* {{ .ExternalURL }}

      {{- end }}


      {{- range .Alerts }}

      {{ template "rancher.text_single" . }}

      {{- end }}

      {{- end -}}


      {{- define "rancher.text_single" -}}

      {{- if .Labels.alertname }}

      *[ALERT - {{ .Labels.alertname }}]*

      {{- else }}

      *[ALERT]*

      {{- end }}

      {{- if .Labels.severity }}

      *Severity:* `{{ .Labels.severity }}`

      {{- end }}

      {{- if .Labels.cluster }}

      *Cluster:*  {{ .Labels.cluster }}

      {{- end }}

      {{- if .Annotations.summary }}

      *Summary:* {{ .Annotations.summary }}

      {{- end }}

      {{- if .Annotations.message }}

      *Message:* {{ .Annotations.message }}

      {{- end }}

      {{- if .Annotations.description }}

      *Description:* {{ .Annotations.description }}

      {{- end }}

      {{- if .Annotations.runbook_url }}

      *Runbook URL:* <{{ .Annotations.runbook_url }}|:spiral_note_pad:>

      {{- end }}

      {{- with .Labels }}

      {{- with .Remove (stringSlice "alertname" "severity" "cluster") }}

      {{- if gt (len .) 0 }}

      *Additional Labels:*
        {{- range .SortedPairs }}
        • *{{ .Name }}:* `{{ .Value }}`
        {{- end }}
      {{- end }}

      {{- end }}

      {{- end }}

      {{- with .Annotations }}

      {{- with .Remove (stringSlice "summary" "message" "description"
      "runbook_url") }}

      {{- if gt (len .) 0 }}

      *Additional Annotations:*
        {{- range .SortedPairs }}
        • *{{ .Name }}:* `{{ .Value }}`
        {{- end }}
      {{- end }}

      {{- end }}

      {{- end }}

      {{- end -}}
  tplConfig: false
global:
  cattle:
    systemDefaultRegistry: ''
    windows:
      enabled: false
  imagePullSecrets: []
  kubectl:
    pullPolicy: IfNotPresent
    repository: rancher/kubectl
    tag: v1.21.5
  rbac:
    create: true
    pspAnnotations: {}
    pspEnabled: true
    userRoles:
      aggregateToDefaultRoles: true
      create: true
grafana:
  additionalDataSources: []
  adminPassword: prom-operator
  defaultDashboards:
    cleanupOnUninstall: false
    namespace: cattle-dashboards
    useExistingNamespace: false
  defaultDashboardsEnabled: true
  defaultDashboardsTimezone: utc
  deploymentStrategy:
    type: Recreate
  enabled: true
  extraConfigmapMounts: []
  extraContainerVolumes:
    - emptyDir: {}
      name: nginx-home
    - configMap:
        items:
          - key: nginx.conf
            mode: 438
            path: nginx.conf
        name: grafana-nginx-proxy-config
      name: grafana-nginx
  extraContainers: |
    - name: grafana-proxy
      args:
      - nginx
      - -g
      - daemon off;
      - -c
      - /nginx/nginx.conf
      image: "{{ template "system_default_registry" . }}{{ .Values.proxy.image.repository }}:{{ .Values.proxy.image.tag }}"
      ports:
      - containerPort: 8080
        name: nginx-http
        protocol: TCP
      volumeMounts:
      - mountPath: /nginx
        name: grafana-nginx
      - mountPath: /var/cache/nginx
        name: nginx-home
      securityContext:
        runAsUser: 101
        runAsGroup: 101
  forceDeployDashboards: false
  forceDeployDatasources: false
  grafana.ini:
    auth:
      disable_login_form: false
    auth.anonymous:
      enabled: true
      org_role: Viewer
    auth.basic:
      enabled: false
    dashboards:
      default_home_dashboard_path: /tmp/dashboards/rancher-default-home.json
    security:
      allow_embedding: true
    users:
      auto_assign_org_role: Viewer
  ingress:
    annotations: {}
    enabled: false
    hosts: []
    labels: {}
    path: /
    tls: []
  namespaceOverride: ''
  proxy:
    image:
      repository: rancher/mirrored-library-nginx
      tag: 1.21.1-alpine
  resources:
    limits:
      cpu: 200m
      memory: 200Mi
    requests:
      cpu: 100m
      memory: 100Mi
  service:
    nodePort: 30950
    port: 80
    portName: nginx-http
    targetPort: 8080
    type: ClusterIP
  serviceMonitor:
    interval: ''
    metricRelabelings: []
    path: /metrics
    relabelings: []
    selfMonitor: true
  sidecar:
    dashboards:
      annotations: {}
      enabled: true
      label: grafana_dashboard
      multicluster:
        etcd:
          enabled: false
        global:
          enabled: false
      provider:
        allowUiUpdates: false
      searchNamespace: cattle-dashboards
    datasources:
      annotations: {}
      createPrometheusReplicasDatasources: false
      defaultDatasourceEnabled: true
      enabled: true
      label: grafana_datasource
  testFramework:
    enabled: false
hardenedKubelet:
  clients:
    https:
      enabled: true
      insecureSkipVerify: true
      useServiceAccountCredentials: true
    port: 10015
    rbac:
      additionalRules:
        - nonResourceURLs:
            - /metrics/cadvisor
          verbs:
            - get
        - apiGroups:
            - ''
          resources:
            - nodes/metrics
          verbs:
            - get
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    useLocalhost: true
  component: kubelet
  enabled: false
  metricsPort: 10250
  serviceMonitor:
    endpoints:
      - honorLabels: true
        port: metrics
        relabelings:
          - sourceLabels:
              - __metrics_path__
            targetLabel: metrics_path
      - honorLabels: true
        path: /metrics/cadvisor
        port: metrics
        relabelings:
          - sourceLabels:
              - __metrics_path__
            targetLabel: metrics_path
      - honorLabels: true
        path: /metrics/probes
        port: metrics
        relabelings:
          - sourceLabels:
              - __metrics_path__
            targetLabel: metrics_path
hardenedNodeExporter:
  clients:
    port: 10016
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    useLocalhost: true
  component: node-exporter
  enabled: false
  metricsPort: 9796
ingressNginx:
  enabled: true
  namespace: kube-system
  service:
    port: 9913
    targetPort: 10254
  serviceMonitor:
    interval: ''
    metricRelabelings: []
    proxyUrl: ''
    relabelings: []
k3sServer:
  clients:
    https:
      enabled: true
      insecureSkipVerify: true
      useServiceAccountCredentials: true
    port: 10013
    rbac:
      additionalRules:
        - nonResourceURLs:
            - /metrics/cadvisor
          verbs:
            - get
        - apiGroups:
            - ''
          resources:
            - nodes/metrics
          verbs:
            - get
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    useLocalhost: true
  component: k3s-server
  enabled: false
  metricsPort: 10250
  serviceMonitor:
    endpoints:
      - honorLabels: true
        port: metrics
        relabelings:
          - sourceLabels:
              - __metrics_path__
            targetLabel: metrics_path
      - honorLabels: true
        path: /metrics/cadvisor
        port: metrics
        relabelings:
          - sourceLabels:
              - __metrics_path__
            targetLabel: metrics_path
      - honorLabels: true
        path: /metrics/probes
        port: metrics
        relabelings:
          - sourceLabels:
              - __metrics_path__
            targetLabel: metrics_path
kube-state-metrics:
  namespaceOverride: ''
  podSecurityPolicy:
    enabled: true
  rbac:
    create: true
  resources:
    limits:
      cpu: 100m
      memory: 200Mi
    requests:
      cpu: 100m
      memory: 130Mi
kubeAdmControllerManager:
  clients:
    https:
      enabled: true
      insecureSkipVerify: true
      useServiceAccountCredentials: true
    nodeSelector:
      node-role.kubernetes.io/master: ''
    port: 10011
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    useLocalhost: true
  component: kube-controller-manager
  enabled: false
  metricsPort: 10257
kubeAdmEtcd:
  clients:
    nodeSelector:
      node-role.kubernetes.io/master: ''
    port: 10014
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    useLocalhost: true
  component: kube-etcd
  enabled: false
  metricsPort: 2381
kubeAdmProxy:
  clients:
    port: 10013
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    useLocalhost: true
  component: kube-proxy
  enabled: false
  metricsPort: 10249
kubeAdmScheduler:
  clients:
    https:
      enabled: true
      insecureSkipVerify: true
      useServiceAccountCredentials: true
    nodeSelector:
      node-role.kubernetes.io/master: ''
    port: 10012
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    useLocalhost: true
  component: kube-scheduler
  enabled: false
  metricsPort: 10259
kubeApiServer:
  enabled: true
  serviceMonitor:
    interval: ''
    jobLabel: component
    metricRelabelings: []
    proxyUrl: ''
    relabelings: []
    selector:
      matchLabels:
        component: apiserver
        provider: kubernetes
  tlsConfig:
    insecureSkipVerify: false
    serverName: kubernetes
kubeControllerManager:
  enabled: false
  endpoints: []
  service:
    enabled: true
    port: 10252
    targetPort: 10252
  serviceMonitor:
    enabled: true
    https: false
    insecureSkipVerify: null
    interval: ''
    metricRelabelings: []
    proxyUrl: ''
    relabelings: []
    serverName: null
kubeDns:
  enabled: false
  service:
    dnsmasq:
      port: 10054
      targetPort: 10054
    skydns:
      port: 10055
      targetPort: 10055
  serviceMonitor:
    dnsmasqMetricRelabelings: []
    dnsmasqRelabelings: []
    interval: ''
    metricRelabelings: []
    proxyUrl: ''
    relabelings: []
kubeEtcd:
  enabled: false
  endpoints: []
  service:
    enabled: true
    port: 2379
    targetPort: 2379
  serviceMonitor:
    caFile: ''
    certFile: ''
    enabled: true
    insecureSkipVerify: false
    interval: ''
    keyFile: ''
    metricRelabelings: []
    proxyUrl: ''
    relabelings: []
    scheme: http
    serverName: ''
kubeProxy:
  enabled: false
  endpoints: []
  service:
    enabled: true
    port: 10249
    targetPort: 10249
  serviceMonitor:
    enabled: true
    https: false
    interval: ''
    metricRelabelings: []
    proxyUrl: ''
    relabelings: []
kubeScheduler:
  enabled: false
  endpoints: []
  service:
    enabled: true
    port: 10251
    targetPort: 10251
  serviceMonitor:
    enabled: true
    https: false
    insecureSkipVerify: null
    interval: ''
    metricRelabelings: []
    proxyUrl: ''
    relabelings: []
    serverName: null
kubeStateMetrics:
  enabled: true
  serviceMonitor:
    honorLabels: true
    interval: ''
    metricRelabelings: []
    proxyUrl: ''
    relabelings: []
    scrapeTimeout: ''
    selectorOverride: {}
    selfMonitor:
      enabled: false
kubeTargetVersionOverride: ''
kubeVersionOverride: ''
kubelet:
  enabled: true
  namespace: kube-system
  serviceMonitor:
    cAdvisor: true
    cAdvisorMetricRelabelings: []
    cAdvisorRelabelings:
      - sourceLabels:
          - __metrics_path__
        targetLabel: metrics_path
    https: true
    interval: ''
    metricRelabelings: []
    probes: true
    probesMetricRelabelings: []
    probesRelabelings:
      - sourceLabels:
          - __metrics_path__
        targetLabel: metrics_path
    proxyUrl: ''
    relabelings:
      - sourceLabels:
          - __metrics_path__
        targetLabel: metrics_path
    resource: false
    resourcePath: /metrics/resource/v1alpha1
    resourceRelabelings:
      - sourceLabels:
          - __metrics_path__
        targetLabel: metrics_path
nameOverride: rancher-monitoring
namespaceOverride: cattle-monitoring-system
nodeExporter:
  enabled: true
  jobLabel: jobLabel
  serviceMonitor:
    interval: ''
    metricRelabelings: []
    proxyUrl: ''
    relabelings: []
    scrapeTimeout: ''
prometheus:
  ingress:
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
    enabled: true
    ingressClassName: nginx
    hosts:
      - prometheus.bp-rke2.marthy.xyz
    tls:
      - secretName: prometheus-tls-secret
        hosts:
          - prometheus.bp-rke2.marthy.xyz
  prometheusSpec:
    containers: |
      - name: prometheus-proxy
        args:
        - nginx
        - -g
        - daemon off;
        - -c
        - /nginx/nginx.conf
        image: "{{ template "system_default_registry" . }}{{ .Values.prometheus.prometheusSpec.proxy.image.repository }}:{{ .Values.prometheus.prometheusSpec.proxy.image.tag }}"
        ports:
        - containerPort: 8081
          name: nginx-http
          protocol: TCP
        volumeMounts:
        - mountPath: /nginx
          name: prometheus-nginx
        - mountPath: /var/cache/nginx
          name: nginx-home
        securityContext:
          runAsUser: 101
          runAsGroup: 101
    image:
      repository: quay.io/prometheus/prometheus
      sha: ''
      tag: v2.34.0
    podMetadata:
      labels:
        thanos-store-api: "true"
    proxy:
      image:
        repository: rancher/mirrored-library-nginx
        tag: 1.21.1-alpine
    query: {}
    queryLogFile: false
    remoteRead: []
    remoteWrite: []
    remoteWriteDashboards: false
    replicaExternalLabelName: ''
    replicaExternalLabelNameClear: false
    replicas: 1
    resources:
      limits:
        cpu: 1000m
        memory: 3000Mi
      requests:
        cpu: 750m
        memory: 750Mi
    retention: 10d
    retentionSize: 18GiB
    routePrefix: /
    ruleNamespaceSelector: {}
    ruleSelector: {}
    ruleSelectorNilUsesHelmValues: false
    scrapeInterval: 1m
    scrapeTimeout: ''
    secrets: []
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
    serviceMonitorNamespaceSelector: {}
    serviceMonitorSelector: {}
    serviceMonitorSelectorNilUsesHelmValues: false
    shards: 1
    storageSpec:
      volumeClaimTemplate:
        metadata:
          name: rancher-prometheus
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 20Gi
    thanos: {}
    tolerations: []
    topologySpreadConstraints: []
    volumeMounts: []
    volumes:
      - emptyDir: {}
        name: nginx-home
      - configMap:
          defaultMode: 438
          name: prometheus-nginx-proxy-config
        name: prometheus-nginx
    walCompression: false
    web: {}
  service:
    annotations: {}
    clusterIP: ''
    externalIPs: []
    labels: {}
    loadBalancerIP: ''
    loadBalancerSourceRanges: []
    nodePort: 30090
    port: 9090
    sessionAffinity: ''
    targetPort: 8081
    type: ClusterIP
  serviceAccount:
    annotations: {}
    create: true
    name: ''
  serviceMonitor:
    bearerTokenFile: null
    interval: ''
    metricRelabelings: []
    relabelings: []
    scheme: ''
    selfMonitor: true
    tlsConfig: {}
  servicePerReplica:
    annotations: {}
    enabled: false
    loadBalancerSourceRanges: []
    nodePort: 30091
    port: 9090
    targetPort: 9090
    type: ClusterIP
  thanosIngress:
    annotations: {}
    enabled: false
    hosts: []
    labels: {}
    nodePort: 30901
    paths: []
    servicePort: 10901
    tls: []
  thanosService:
    annotations: {}
    clusterIP: None
    enabled: false
    httpNodePort: 30902
    httpPort: 10902
    httpPortName: http
    labels: {}
    nodePort: 30901
    port: 10901
    portName: grpc
    targetHttpPort: http
    targetPort: grpc
    type: ClusterIP
  thanosServiceExternal:
    annotations: {}
    enabled: false
    httpNodePort: 30902
    httpPort: 10902
    httpPortName: http
    labels: {}
    loadBalancerIP: ''
    loadBalancerSourceRanges: []
    nodePort: 30901
    port: 10901
    portName: grpc
    targetHttpPort: http
    targetPort: grpc
    type: LoadBalancer
  thanosServiceMonitor:
    bearerTokenFile: null
    enabled: false
    interval: ''
    metricRelabelings: []
    relabelings: []
    scheme: ''
    tlsConfig: {}
prometheus-adapter:
  enabled: true
  prometheus:
    port: 9090
    url: http://rancher-monitoring-prometheus.cattle-monitoring-system.svc
  psp:
    create: true
prometheus-node-exporter:
  extraArgs:
    - >-
      --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
    - >-
      --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
  namespaceOverride: ''
  podLabels:
    jobLabel: node-exporter
  resources:
    limits:
      cpu: 200m
      memory: 50Mi
    requests:
      cpu: 100m
      memory: 30Mi
  service:
    port: 9796
    targetPort: 9796
prometheusOperator:
  image:
    repository: quay.io/prometheus-operator/prometheus-operator
    tag: v0.54.0
  prometheusConfigReloaderImage:
    repository: quay.io/prometheus-operator/prometheus-config-reloader
    tag: v0.54.0
  thanosImage:
    repository: quay.io/thanos/thanos
    tag: v0.24.0
  admissionWebhooks:
    caBundle: ''
    certManager:
      enabled: false
    enabled: true
    failurePolicy: Fail
    patch:
      affinity: {}
      enabled: true
      image:
        pullPolicy: IfNotPresent
        repository: rancher/mirrored-ingress-nginx-kube-webhook-certgen
        sha: ''
        tag: v1.0
      nodeSelector: {}
      podAnnotations: {}
      priorityClassName: ''
      resources: {}
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
      tolerations: []
  affinity: {}
  alertmanagerInstanceNamespaces: []
  configReloaderCpu: 100m
  configReloaderMemory: 50Mi
  denyNamespaces: []
  dnsConfig: {}
  enabled: true
  hostNetwork: false
  kubeletService:
    enabled: true
    namespace: kube-system
  namespaces: {}
  nodeSelector: {}
  podAnnotations: {}
  podLabels: {}
  prometheusInstanceNamespaces: []
  resources:
    limits:
      cpu: 200m
      memory: 500Mi
    requests:
      cpu: 100m
      memory: 100Mi
  secretFieldSelector: ''
  securityContext:
    fsGroup: 65534
    runAsGroup: 65534
    runAsNonRoot: true
    runAsUser: 65534
  service:
    additionalPorts: []
    annotations: {}
    clusterIP: ''
    externalIPs: []
    labels: {}
    loadBalancerIP: ''
    loadBalancerSourceRanges: []
    nodePort: 30080
    nodePortTls: 30443
    type: ClusterIP
  serviceAccount:
    create: true
    name: ''
  serviceMonitor:
    interval: ''
    metricRelabelings: []
    relabelings: []
    scrapeTimeout: ''
    selfMonitor: true
  thanosRulerInstanceNamespaces: []
  tls:
    enabled: true
    internalPort: 8443
    tlsMinVersion: VersionTLS13
  tolerations: []
rke2ControllerManager:
  clients:
    nodeSelector:
      node-role.kubernetes.io/master: 'true'
    port: 10011
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    useLocalhost: true
  component: kube-controller-manager
  enabled: true
  metricsPort: 10252
rke2Etcd:
  clients:
    nodeSelector:
      node-role.kubernetes.io/etcd: 'true'
    port: 10014
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    useLocalhost: true
  component: kube-etcd
  enabled: true
  metricsPort: 2381
rke2IngressNginx:
  clients:
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                    - controller
            namespaces:
              - kube-system
            topologyKey: kubernetes.io/hostname
    deployment:
      enabled: false
      replicas: 1
    port: 10015
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    useLocalhost: true
  component: ingress-nginx
  enabled: false
  metricsPort: 10254
rke2Proxy:
  clients:
    port: 10013
    useLocalhost: true
  component: kube-proxy
  enabled: true
  metricsPort: 10249
  tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
rke2Scheduler:
  clients:
    nodeSelector:
      node-role.kubernetes.io/master: 'true'
    port: 10012
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    useLocalhost: true
  component: kube-scheduler
  enabled: true
  metricsPort: 10251
rkeControllerManager:
  clients:
    nodeSelector:
      node-role.kubernetes.io/controlplane: 'true'
    port: 10011
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    useLocalhost: true
  component: kube-controller-manager
  enabled: false
  metricsPort: 10252
rkeEtcd:
  clients:
    https:
      caCertFile: kube-ca.pem
      certDir: /etc/kubernetes/ssl
      certFile: kube-etcd-*.pem
      enabled: true
      keyFile: kube-etcd-*-key.pem
    nodeSelector:
      node-role.kubernetes.io/etcd: 'true'
    port: 10014
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
  component: kube-etcd
  enabled: false
  metricsPort: 2379
rkeIngressNginx:
  clients:
    nodeSelector:
      node-role.kubernetes.io/worker: 'true'
    port: 10015
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    useLocalhost: true
  component: ingress-nginx
  enabled: false
  metricsPort: 10254
rkeProxy:
  clients:
    port: 10013
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    useLocalhost: true
  component: kube-proxy
  enabled: false
  metricsPort: 10249
rkeScheduler:
  clients:
    nodeSelector:
      node-role.kubernetes.io/controlplane: 'true'
    port: 10012
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    useLocalhost: true
  component: kube-scheduler
  enabled: false
  metricsPort: 10251